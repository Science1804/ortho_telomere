{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61736fb-3d7e-42d4-a0d8-b14b8c337d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce24d5c-5a16-4b79-8a82-eb43525a5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "170cda0c-1325-472d-b8b9-8feb1dd73fee",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (1914429242.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [180]\u001b[0;36m\u001b[0m\n\u001b[0;31m    )\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super(VAE, self).__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "                name=\"reconstruction_loss\"\n",
    "            )\n",
    "            self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                reconstruction_loss = tf.keras.losses.MeanSquaredError(data, reconstruction)\n",
    "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "                total_loss = reconstruction_loss + kl_loss\n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e01b7556-0718-4ae7-ab10-4f001b4ed1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAED2:\n",
    "    \n",
    "    class Sampling(tf.keras.layers.Layer):\n",
    "        \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    \n",
    "    def __init__(self,neurons_in_layers,lower_dim,input_shape=(2830),acv='relu',reg='l2'):\n",
    "            \n",
    "        self.neurons_in_layers = neurons_in_layers \n",
    "        self.input_shape = input_shape\n",
    "        self.lower_dim = lower_dim\n",
    "        self.acv = acv\n",
    "        self.reg = reg\n",
    "        return\n",
    "\n",
    "    def Encoder(self):\n",
    "        \"layers_filter should be a list: [32,64,64] \"\n",
    "        inputs = tf.keras.layers.Input(shape=self.input_shape,name='Input')\n",
    "        x = inputs\n",
    "        for neurons in self.neurons_in_layers:\n",
    "            x = tf.keras.layers.Dense(units=neurons,activation=self.acv,kernel_regularizer=self.reg)(x)\n",
    "        z_mean = tf.keras.layers.Dense(self.lower_dim, name=\"z_mean\")(x)\n",
    "        z_log_var = tf.keras.layers.Dense(self.lower_dim, name=\"z_log_var\")(x)\n",
    "        z = Sampling()([z_mean, z_log_var])\n",
    "        encoder = tf.keras.Model(inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "        return encoder\n",
    "\n",
    "\n",
    "    def Decoder(self,enc_shape):\n",
    "        \"layers_filter should be a list: [32,64,64] given as same to encoder \"\n",
    "        self.neurons_in_layers.reverse();\n",
    "        lower_inputs = tf.keras.layers.Input(shape=self.lower_dim,name='Decoder_Input')\n",
    "        x = tf.keras.layers.Dense(enc_shape[0],activation=self.acv,kernel_regularizer=self.reg)(lower_inputs)\n",
    "        for i,neurons in enumerate(self.neurons_in_layers[:]):\n",
    "            x = tf.keras.layers.Dense(units=neurons,activation=self.acv,kernel_regularizer=self.reg)(x)\n",
    "            \n",
    "        outputs = x\n",
    "\n",
    "        decoder = tf.keras.Model(inputs=lower_inputs,outputs=outputs, name='Decoder')\n",
    "\n",
    "        return decoder   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "52ee00f6-dc12-4f0d-afdc-796e45038928",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAED2([10,30],2,(10),'selu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "d7699b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = vae.Encoder();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "aca3155c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input (InputLayer)             [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " dense_101 (Dense)              (None, 10)           110         ['Input[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_102 (Dense)              (None, 30)           330         ['dense_101[0][0]']              \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            62          ['dense_102[0][0]']              \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            62          ['dense_102[0][0]']              \n",
      "                                                                                                  \n",
      " sampling_15 (Sampling)         (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 564\n",
      "Trainable params: 564\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "11ba1485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Decoder_Input (InputLayer)  [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 2)                 6         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 30)                90        \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 10)                310       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 406\n",
      "Trainable params: 406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = vae.Decoder(encoder.layers[-2].output_shape[1:])\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "c2e9124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f84f3f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.load('ar1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f853d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for j in range(1000):\n",
    "    x = []\n",
    "    for i in range(10):\n",
    "        x.append(i*2)\n",
    "    data.append(x)\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4600d5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ad4d8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "979d4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "08095b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 1s 2ms/step - loss: 3075.1403 - reconstruction_loss: 2498.3491 - kl_loss: 72.0280\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1438.5555 - reconstruction_loss: 1090.4528 - kl_loss: 149.0403\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 749.2606 - reconstruction_loss: 481.5361 - kl_loss: 128.3701\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 319.7309 - reconstruction_loss: 142.8595 - kl_loss: 133.4518\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 191.0244 - reconstruction_loss: 50.4688 - kl_loss: 117.3307\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 114.0759 - reconstruction_loss: 9.5685 - kl_loss: 98.2488\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 91.2823 - reconstruction_loss: 5.7941 - kl_loss: 81.5597\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 76.7681 - reconstruction_loss: 4.4390 - kl_loss: 69.7238\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 66.6819 - reconstruction_loss: 3.5145 - kl_loss: 61.0274\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 58.5388 - reconstruction_loss: 2.8539 - kl_loss: 53.9712\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 51.8673 - reconstruction_loss: 2.3953 - kl_loss: 48.0735\n",
      "Epoch 12/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 46.6596 - reconstruction_loss: 2.3301 - kl_loss: 43.2945\n",
      "Epoch 13/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 42.4843 - reconstruction_loss: 2.1662 - kl_loss: 39.3530\n",
      "Epoch 14/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 38.6557 - reconstruction_loss: 1.9161 - kl_loss: 36.0072\n",
      "Epoch 15/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 35.6001 - reconstruction_loss: 1.6993 - kl_loss: 33.2287\n",
      "Epoch 16/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 32.9825 - reconstruction_loss: 1.6075 - kl_loss: 30.7432\n",
      "Epoch 17/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 30.7921 - reconstruction_loss: 1.5201 - kl_loss: 28.7641\n",
      "Epoch 18/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 28.6182 - reconstruction_loss: 1.5035 - kl_loss: 26.7501\n",
      "Epoch 19/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 27.1051 - reconstruction_loss: 1.4885 - kl_loss: 25.1530\n",
      "Epoch 20/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 25.3217 - reconstruction_loss: 1.3572 - kl_loss: 23.6752\n",
      "Epoch 21/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 24.0799 - reconstruction_loss: 1.3442 - kl_loss: 22.3601\n",
      "Epoch 22/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 22.8343 - reconstruction_loss: 1.3732 - kl_loss: 21.1567\n",
      "Epoch 23/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 21.6941 - reconstruction_loss: 1.2536 - kl_loss: 20.2068\n",
      "Epoch 24/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 20.7008 - reconstruction_loss: 1.3620 - kl_loss: 19.1502\n",
      "Epoch 25/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 19.9590 - reconstruction_loss: 1.4103 - kl_loss: 18.3442\n",
      "Epoch 26/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 19.0360 - reconstruction_loss: 1.2324 - kl_loss: 17.5992\n",
      "Epoch 27/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 18.2664 - reconstruction_loss: 1.1489 - kl_loss: 16.8479\n",
      "Epoch 28/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 17.6989 - reconstruction_loss: 1.2389 - kl_loss: 16.2267\n",
      "Epoch 29/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 16.9792 - reconstruction_loss: 1.2321 - kl_loss: 15.5721\n",
      "Epoch 30/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 16.4230 - reconstruction_loss: 1.2034 - kl_loss: 15.0380\n",
      "Epoch 31/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 15.7564 - reconstruction_loss: 1.2302 - kl_loss: 14.5383\n",
      "Epoch 32/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 15.4214 - reconstruction_loss: 1.2657 - kl_loss: 13.9806\n",
      "Epoch 33/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 14.9307 - reconstruction_loss: 1.1928 - kl_loss: 13.5197\n",
      "Epoch 34/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 14.4405 - reconstruction_loss: 1.2483 - kl_loss: 13.1020\n",
      "Epoch 35/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 14.0723 - reconstruction_loss: 1.1779 - kl_loss: 12.7596\n",
      "Epoch 36/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 14.4779 - reconstruction_loss: 1.7658 - kl_loss: 12.4302\n",
      "Epoch 37/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 13.5867 - reconstruction_loss: 1.2145 - kl_loss: 12.0634\n",
      "Epoch 38/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 13.0650 - reconstruction_loss: 1.3598 - kl_loss: 11.7217\n",
      "Epoch 39/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 13.1786 - reconstruction_loss: 1.7196 - kl_loss: 11.4708\n",
      "Epoch 40/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 12.8288 - reconstruction_loss: 1.4285 - kl_loss: 11.1307\n",
      "Epoch 41/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 12.1739 - reconstruction_loss: 1.2774 - kl_loss: 10.9330\n",
      "Epoch 42/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 12.1324 - reconstruction_loss: 1.2520 - kl_loss: 10.6169\n",
      "Epoch 43/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 11.7512 - reconstruction_loss: 1.1870 - kl_loss: 10.3854\n",
      "Epoch 44/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 11.5759 - reconstruction_loss: 1.2356 - kl_loss: 10.1559\n",
      "Epoch 45/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 11.4552 - reconstruction_loss: 1.3540 - kl_loss: 9.9227\n",
      "Epoch 46/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 11.1203 - reconstruction_loss: 1.1897 - kl_loss: 9.7988\n",
      "Epoch 47/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 10.7386 - reconstruction_loss: 1.1827 - kl_loss: 9.5772\n",
      "Epoch 48/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 10.9947 - reconstruction_loss: 1.2422 - kl_loss: 9.5099\n",
      "Epoch 49/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 10.9130 - reconstruction_loss: 1.1805 - kl_loss: 9.3275\n",
      "Epoch 50/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 10.3075 - reconstruction_loss: 1.2183 - kl_loss: 9.0538\n",
      "Epoch 51/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 10.5121 - reconstruction_loss: 1.2692 - kl_loss: 9.0107\n",
      "Epoch 52/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.9882 - reconstruction_loss: 1.2494 - kl_loss: 8.7620\n",
      "Epoch 53/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 10.2198 - reconstruction_loss: 1.4868 - kl_loss: 8.6372\n",
      "Epoch 54/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 9.8274 - reconstruction_loss: 1.2796 - kl_loss: 8.5080\n",
      "Epoch 55/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.8816 - reconstruction_loss: 1.2952 - kl_loss: 8.4526\n",
      "Epoch 56/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 10.1720 - reconstruction_loss: 1.4339 - kl_loss: 8.3069\n",
      "Epoch 57/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 9.5084 - reconstruction_loss: 1.2166 - kl_loss: 8.2153\n",
      "Epoch 58/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.5703 - reconstruction_loss: 1.4291 - kl_loss: 8.1101\n",
      "Epoch 59/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 9.2567 - reconstruction_loss: 1.1659 - kl_loss: 8.0070\n",
      "Epoch 60/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 9.3363 - reconstruction_loss: 1.2716 - kl_loss: 7.8556\n",
      "Epoch 61/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 9.4009 - reconstruction_loss: 1.3990 - kl_loss: 7.6995\n",
      "Epoch 62/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 9.0186 - reconstruction_loss: 1.1704 - kl_loss: 7.7461\n",
      "Epoch 63/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8.8653 - reconstruction_loss: 1.1803 - kl_loss: 7.6562\n",
      "Epoch 64/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8.8112 - reconstruction_loss: 1.2492 - kl_loss: 7.4952\n",
      "Epoch 65/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8.4958 - reconstruction_loss: 1.1246 - kl_loss: 7.3605\n",
      "Epoch 66/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8.5150 - reconstruction_loss: 1.1395 - kl_loss: 7.3214\n",
      "Epoch 67/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8.4806 - reconstruction_loss: 1.1463 - kl_loss: 7.2878\n",
      "Epoch 68/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8.3908 - reconstruction_loss: 1.1576 - kl_loss: 7.1776\n",
      "Epoch 69/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.4560 - reconstruction_loss: 1.2825 - kl_loss: 7.1117\n",
      "Epoch 70/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.1094 - reconstruction_loss: 1.0567 - kl_loss: 7.0665\n",
      "Epoch 71/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.3319 - reconstruction_loss: 1.1136 - kl_loss: 7.0319\n",
      "Epoch 72/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8.3513 - reconstruction_loss: 1.3138 - kl_loss: 6.9363\n",
      "Epoch 73/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8.1530 - reconstruction_loss: 1.1010 - kl_loss: 6.7996\n",
      "Epoch 74/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.8968 - reconstruction_loss: 1.1660 - kl_loss: 6.7373\n",
      "Epoch 75/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.9037 - reconstruction_loss: 1.0830 - kl_loss: 6.7759\n",
      "Epoch 76/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.9657 - reconstruction_loss: 1.1916 - kl_loss: 6.6928\n",
      "Epoch 77/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.1487 - reconstruction_loss: 1.3469 - kl_loss: 6.6549\n",
      "Epoch 78/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.9171 - reconstruction_loss: 1.1359 - kl_loss: 6.6342\n",
      "Epoch 79/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.7218 - reconstruction_loss: 1.3297 - kl_loss: 6.4733\n",
      "Epoch 80/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 8.0625 - reconstruction_loss: 1.4224 - kl_loss: 6.4791\n",
      "Epoch 81/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.7594 - reconstruction_loss: 1.2422 - kl_loss: 6.4103\n",
      "Epoch 82/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.6707 - reconstruction_loss: 1.3488 - kl_loss: 6.2900\n",
      "Epoch 83/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.4902 - reconstruction_loss: 1.1031 - kl_loss: 6.3764\n",
      "Epoch 84/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.4671 - reconstruction_loss: 1.1981 - kl_loss: 6.2935\n",
      "Epoch 85/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.4825 - reconstruction_loss: 1.1411 - kl_loss: 6.3362\n",
      "Epoch 86/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 8.2839 - reconstruction_loss: 1.5947 - kl_loss: 6.2463\n",
      "Epoch 87/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.2135 - reconstruction_loss: 1.0152 - kl_loss: 6.1540\n",
      "Epoch 88/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.4376 - reconstruction_loss: 1.2419 - kl_loss: 6.1334\n",
      "Epoch 89/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.7098 - reconstruction_loss: 1.3242 - kl_loss: 6.1034\n",
      "Epoch 90/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.2559 - reconstruction_loss: 1.1689 - kl_loss: 6.0483\n",
      "Epoch 91/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.4762 - reconstruction_loss: 1.2871 - kl_loss: 6.0277\n",
      "Epoch 92/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.4459 - reconstruction_loss: 1.2207 - kl_loss: 6.0940\n",
      "Epoch 93/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.2102 - reconstruction_loss: 1.1515 - kl_loss: 5.9645\n",
      "Epoch 94/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.2741 - reconstruction_loss: 1.2322 - kl_loss: 6.0173\n",
      "Epoch 95/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6.9818 - reconstruction_loss: 1.1049 - kl_loss: 5.8929\n",
      "Epoch 96/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6.9553 - reconstruction_loss: 1.0029 - kl_loss: 5.8961\n",
      "Epoch 97/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.1527 - reconstruction_loss: 1.3936 - kl_loss: 5.8088\n",
      "Epoch 98/100\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.1844 - reconstruction_loss: 1.2134 - kl_loss: 5.8950\n",
      "Epoch 99/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 7.1226 - reconstruction_loss: 1.2028 - kl_loss: 5.8989\n",
      "Epoch 100/100\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 6.9539 - reconstruction_loss: 1.0782 - kl_loss: 5.7983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f526cfccaf0>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f41ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "897df2e3b593fd048babee75ad31ed35af326453db3a203f11e4ce60aad188e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
